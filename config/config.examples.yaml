# =============================================================================
# FluxForge Configuration Examples
# =============================================================================
# This file demonstrates various storage configuration scenarios.
# Copy the relevant example to config/config.yaml and customize.

# =============================================================================
# EXAMPLE 1: ALL-LOCAL STORAGE (Development/Testing)
# =============================================================================
# Use this for local development, testing, or single-machine deployments.
# All data stays on local filesystem.

---
storage:
  # Ingestion layer storage (where raw segments are written)
  ingestion_storage:
    backend: "local"
    base_dir: "F:/"  # Or "./data" for relative path
  
  # ETL input storage (where ETL reads raw segments from)
  etl_storage_input:
    backend: "local"
    base_dir: "F:/"
  
  # ETL output storage (where processed Parquet files are written)
  etl_storage_output:
    backend: "local"
    base_dir: "F:/"

  # Path structure - customize your directory layout
  paths:
    raw_dir: "raw"
    active_subdir: "active"
    ready_subdir: "ready"
    processing_subdir: "processing"
    processed_dir: "processed"

# Data Sources
coinbase:
  api_key: "your-api-key"
  api_secret: "your-api-secret"
  product_ids:
    - "BTC-USD"
    - "ETH-USD"
    - "SOL-USD"
  channels:
    - "ticker"
    - "level2"
    - "market_trades"
  ws_url: "wss://advanced-trade-ws.coinbase.com"
  level2_batch_size: 10

ccxt:
  exchanges:
    binance:
      api_key: ""
      api_secret: ""
      channels:
        watchTicker: ["BTC/USDT", "ETH/USDT"]
        watchTrades: ["BTC/USDT"]
    okex:
      api_key: ""
      api_secret: ""
      password: ""
      channels:
        watchOrderBook: ["BTC/USDT"]

databento:
  api_key: "your-databento-key"
  dataset: "XNAS.ITCH"
  symbols: ["AAPL", "MSFT", "TSLA"]
  schema: "trades"

ibkr:
  gateway_url: "https://localhost:5000"
  account_id: "your-account-id"
  contracts: []

# Ingestion Settings
ingestion:
  batch_size: 100
  flush_interval_seconds: 5.0
  queue_maxsize: 10000
  enable_fsync: true
  auto_reconnect: true
  max_reconnect_attempts: 10
  reconnect_delay: 5.0
  segment_max_mb: 100

# ETL Settings
etl:
  compression: "snappy"
  schedule_cron: null
  delete_after_processing: true
  
  channels:
    level2:
      partition_cols: ["product_id", "date"]
      processor_options:
        reconstruct_lob: false
        compute_features: false
        add_derived_fields: true
    
    market_trades:
      partition_cols: ["product_id", "date"]
      processor_options:
        add_derived_fields: true
        infer_aggressor: false
    
    ticker:
      partition_cols: ["product_id", "date"]
      processor_options:
        add_derived_fields: true

log_level: "INFO"
log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"


# =============================================================================
# EXAMPLE 2: HYBRID STORAGE (Local Ingest -> S3 ETL)
# =============================================================================
# Recommended for production: Fast local ingestion, durable S3 storage.

---
storage:
  ingestion_storage:
    backend: "local"
    base_dir: "/mnt/fast-ssd/data"
  
  etl_storage_input:
    backend: "local"
    base_dir: "/mnt/fast-ssd/data"
  
  etl_storage_output:
    backend: "s3"
    base_dir: "market-data-vault"
    s3:
      bucket: "market-data-vault"
      region: "us-east-1"
      # Use IAM roles in production instead of explicit keys
      aws_access_key_id: null
      aws_secret_access_key: null

  paths:
    raw_dir: "raw"
    active_subdir: "active"
    ready_subdir: "ready"
    processing_subdir: "processing"
    processed_dir: "processed"

coinbase:
  api_key: "your-api-key"
  api_secret: "your-api-secret"
  product_ids: ["BTC-USD"]
  channels: ["ticker", "level2", "market_trades"]
  ws_url: "wss://advanced-trade-ws.coinbase.com"
  level2_batch_size: 10

ingestion:
  batch_size: 100
  flush_interval_seconds: 5.0
  segment_max_mb: 100

etl:
  compression: "snappy"
  delete_after_processing: true
  channels:
    level2:
      partition_cols: ["product_id", "date"]
      processor_options: {add_derived_fields: true}
    market_trades:
      partition_cols: ["product_id", "date"]
      processor_options: {add_derived_fields: true}
    ticker:
      partition_cols: ["product_id", "date"]
      processor_options: {add_derived_fields: true}

log_level: "INFO"
log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"


# =============================================================================
# EXAMPLE 3: ALL-S3 STORAGE (Cloud Native)
# =============================================================================
# Fully cloud-native deployment.

---
storage:
  ingestion_storage:
    backend: "s3"
    base_dir: "market-data-vault"
    s3:
      bucket: "market-data-vault"
      region: "us-east-1"
  
  etl_storage_input:
    backend: "s3"
    base_dir: "market-data-vault"
    s3:
      bucket: "market-data-vault"
      region: "us-east-1"
  
  etl_storage_output:
    backend: "s3"
    base_dir: "market-data-vault"
    s3:
      bucket: "market-data-vault"
      region: "us-east-1"

  paths:
    raw_dir: "raw"
    active_subdir: "active"
    ready_subdir: "ready"
    processing_subdir: "processing"
    processed_dir: "processed"

coinbase:
  api_key: "your-api-key"
  api_secret: "your-api-secret"
  product_ids: ["BTC-USD"]
  channels: ["ticker", "level2", "market_trades"]
  ws_url: "wss://advanced-trade-ws.coinbase.com"
  level2_batch_size: 10

ingestion:
  batch_size: 100
  flush_interval_seconds: 5.0
  segment_max_mb: 100

etl:
  compression: "snappy"
  delete_after_processing: true
  channels:
    level2:
      partition_cols: ["product_id", "date"]
      processor_options: {add_derived_fields: true}
    market_trades:
      partition_cols: ["product_id", "date"]
      processor_options: {add_derived_fields: true}
    ticker:
      partition_cols: ["product_id", "date"]
      processor_options: {add_derived_fields: true}

log_level: "INFO"
log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
