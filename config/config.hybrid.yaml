# FluxForge Hybrid Storage Configuration Example
# Ingest to local storage (fast), ETL to S3 (durable)

# =============================================================================
# HYBRID STORAGE CONFIGURATION
# =============================================================================
storage:
  # Hybrid mode: Separate storage backends for ingestion and ETL
  # Ingestion writes to fast local storage
  # ETL reads from local, writes processed data to durable S3
  
  # Ingestion storage (local for speed)
  ingestion_storage:
    backend: "local"
    base_dir: "F:/"
    # S3 config not needed for local
  
  # ETL input storage (same as ingestion - read from local)
  etl_storage_input:
    backend: "local"
    base_dir: "F:/"
  
  # ETL output storage (S3 for durability and scalability)
  etl_storage_output:
    backend: "s3"
    base_dir: "market-data-vault"
    s3:
      bucket: "market-data-vault"
      region: null  # Auto-detected
      aws_access_key_id: null  # Uses environment/IAM
      aws_secret_access_key: null
  
  # Path structure (applies to all storage backends)
  paths:
    raw_dir: "raw"
    active_subdir: "active"
    ready_subdir: "ready"
    processing_subdir: "processing"
    processed_dir: "processed"

# =============================================================================
# DATA SOURCES
# =============================================================================
coinbase:
  api_key: "your-api-key"
  api_secret: "your-api-secret"
  product_ids:
    - "BTC-USD"
    - "ETH-USD"
  channels:
    - "ticker"
    - "level2"
    - "market_trades"
  ws_url: "wss://advanced-trade-ws.coinbase.com"
  level2_batch_size: 10

# =============================================================================
# INGESTION SETTINGS
# =============================================================================
ingestion:
  batch_size: 100
  flush_interval_seconds: 5.0
  queue_maxsize: 10000
  enable_fsync: true
  segment_max_mb: 100

# =============================================================================
# ETL SETTINGS
# =============================================================================
etl:
  compression: "snappy"
  delete_after_processing: true
  
  channels:
    level2:
      partition_cols:
        - "product_id"
        - "date"
      processor_options:
        add_derived_fields: true
    
    market_trades:
      partition_cols:
        - "product_id"
        - "date"
      processor_options:
        add_derived_fields: true
    
    ticker:
      partition_cols:
        - "product_id"
        - "date"
      processor_options:
        add_derived_fields: true

# =============================================================================
# LOGGING
# =============================================================================
log_level: "INFO"
log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# =============================================================================
# EXPECTED DATA FLOW
# =============================================================================
# 
# 1. Ingestion (writes to local F:/)
#    - Active segments: F:/raw/active/coinbase/segment_*.ndjson
#    - Ready segments:  F:/raw/ready/coinbase/segment_*.ndjson
# 
# 2. ETL (reads from local F:/, writes to S3)
#    - Input:  F:/raw/ready/coinbase/segment_*.ndjson (local)
#    - Output: s3://market-data-vault/processed/coinbase/**/*.parquet (S3)
# 
# Benefits:
# - Fast ingestion to local SSD (no network latency)
# - Durable storage of processed data in S3
# - Cost-effective: Only S3 storage costs, no S3 PUT costs for raw data
# - Local raw data can be deleted after ETL to save disk space
